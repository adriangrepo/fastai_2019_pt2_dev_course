{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio classification the from-scratch way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks to the SF Study Group practitioners: @aamir7117, @marii, @simonjhb, @ste, @ThomM, @zachcaceres."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to demonstrate the technique of classifying audio samples by first converting the audio into spectrograms, then treating the spectrograms as images. Once we've converted the spectrograms to images, the workflow is just the same as using imagenette or any other image classification task.\n",
    "\n",
    "What do we need to do?\n",
    "* Download the data\n",
    "* Load the data \n",
    "* Transform the data into spectrograms\n",
    "* Load the audio data into a databunch such that we can use our previously-defined `learner` object\n",
    "\n",
    "Still to come - data augmentations for audio, 1D convolutional models, RNNs with audio… and more, with your contribution :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup & imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We rely heavily on [torchaudio](https://github.com/pytorch/audio) - which you'll have to compile to install."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from exp.nb_12c import *\n",
    "\n",
    "import torchaudio\n",
    "from torchaudio import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "AUDIO_EXTS = {str.lower(k) for k,v in mimetypes.types_map.items() if v.startswith('audio/')}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should be one line; it's only so complicated because the target .tgz file doesn't extract itself to its own directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsid = \"ST-AEDS-20180100_1-OS\"\n",
    "data_url = f'http://www.openslr.org/resources/45/{dsid}' # actual URL has .tgz extension but untar_data doesn't like that\n",
    "path = Path.home() / Path(f\".fastai/data/{dsid}/\")\n",
    "datasets.untar_data(data_url, dest=path)\n",
    "path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading into an AudioList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting a file list the `08_data_block` way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"manual\" way using `get_files`…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audios = get_files(path, extensions=AUDIO_EXTS)\n",
    "print(f\"Found {len(audios)} audio files\")\n",
    "audios[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "…But that's not very exciting. Let's make an `AudioList`, so we can use transforms, and define how to `get` an Audio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AudioList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class AudioList(ItemList):\n",
    "    @classmethod\n",
    "    def from_files(cls, path, extensions=None, recurse=True, include=None, **kwargs):\n",
    "        if extensions is None: extensions = AUDIO_EXTS\n",
    "        return cls(get_files(path, extensions, recurse=recurse, include=include), path, **kwargs)\n",
    "    \n",
    "    def get(self, fn): \n",
    "        sig, sr = torchaudio.load(fn)\n",
    "        assert sig.size(0) == 1, \"Non-mono audio detected, mono only supported for now.\"\n",
    "        return (sig, sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "al = AudioList.from_files(path); al"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like this is full of file paths, but that's just the `repr` talking. Actually accessing an item from the list calls the `get` method and returns a `(Tensor, Int)` tuple representing the signal & sample rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "al[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting into train/validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data is all in one folder, there's no specific validation set, so let's just split it at random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd = SplitData.split_by_func(al, partial(random_splitter, p_valid=0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our labels are encoded in our filenames. For example, `m0003_us_m0003_00032.wav` has the label `m0003`. Let's make a regex labeler, then use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def re_labeler(fn, pat): return re.findall(pat, str(fn))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_pat = r'/([mf]\\d+)_'\n",
    "speaker_labeler = partial(re_labeler, pat=label_pat)\n",
    "ll = label_by_func(sd, speaker_labeler, proc_y=CategoryProcessor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforms: audio clipping & conversion to spectrograms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pytorch dataloader needs to be all tensors to be the same size, but our input audio files are of different sizes, so we need to trim them. Also, recall that we're not going to send the model the _audio_ directly; we're going to convert it to spectrograms first. We can treat these steps as transforms. In particular, the `_order` property makes this simple."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### toCuda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other transforms both use all-tensor ops, so it should help. Let's try it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ToCuda(Transform):\n",
    "    _order=10\n",
    "    def __call__(self, ad):\n",
    "        sig,sr=ad\n",
    "        return (sig.cuda(), sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ToCuda()(ll.train[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PadOrTrim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torchaudio` has one for this already; all we're doing is taking an argument in milliseconds rather than frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class PadOrTrim(Transform):\n",
    "    _order=11\n",
    "    def __init__(self,msecs):\n",
    "        self.msecs = msecs\n",
    "        \n",
    "    def __call__(self, ad): \n",
    "        sig, sr = ad\n",
    "        mx = sr//1000 * self.msecs\n",
    "        return (transforms.PadTrim(mx)(sig), sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Small helper to show some audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from IPython.display import Audio\n",
    "def show_audio(ad):\n",
    "    sig,sr=ad\n",
    "    return Audio(data=sig, rate=sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note - this won't work if you've already run the notebook all the way through, because `ll` now contains Tensors representing Spectrograms, not `(Signal, SampleRate)` tuples.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_audio(ll.train[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt = PadOrTrim(3000) ## duration in milliseconds\n",
    "show_audio(pt(ll.train[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spectrogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luckily, `torchaudio` takes care of the calculation & conversion to Spectrograms for us.\n",
    "\n",
    "Instead of clipping our audio, we could also modify our spectrogram transform to ensure all the final spectrograms had the same shape, but that's a little more complicated, as the size of the spectrograms is a function of the length of the clip; so we'd have to calculate a `n_mels` and/or `ws` (window_size) param independently per clip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Spectrogrammer(Transform):\n",
    "    _order=90\n",
    "    def __init__(self, to_mel=True, to_db=True, n_fft=400, ws=None, hop=None, \n",
    "                 f_min=0.0, f_max=None, pad=0, n_mels=128, top_db=None, normalize=False):\n",
    "        self.to_mel, self.to_db, self.n_fft, self.ws, self.hop, self.f_min, self.f_max, \\\n",
    "        self.pad, self.n_mels, self.top_db, self.normalize = to_mel, to_db, n_fft, \\\n",
    "        ws, hop, f_min, f_max, pad, n_mels, top_db, normalize\n",
    "\n",
    "    def __call__(self, ad):\n",
    "        sig,sr = ad\n",
    "        if self.to_mel:\n",
    "            spec = transforms.MelSpectrogram(sr, self.n_fft, self.ws, self.hop, self.f_min, \n",
    "                                             self.f_max, self.pad, self.n_mels)(sig)\n",
    "        else: \n",
    "            spec = transforms.Spectrogram(self.n_fft, self.ws, self.hop, self.pad, \n",
    "                                          normalize=self.normalize)(sig)\n",
    "        if self.to_db:\n",
    "            spec = transforms.SpectrogramToDB(top_db=self.top_db)(spec)\n",
    "        spec = spec.permute(0,2,1)\n",
    "        return spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speccer = Spectrogrammer(to_db=True, n_fft=1024, n_mels=64, top_db=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Small helper to show a spectrogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def show_spectro(img, ax=None, figsize=(6,6), with_shape=True):\n",
    "    if hasattr(img,\"device\") & str(img.device).startswith(\"cuda\"): img = img.cpu()\n",
    "    if ax is None: _,ax = plt.subplots(1, 1, figsize=figsize)\n",
    "    ax.imshow(img if (img.shape[0]==3) else img.squeeze(0))\n",
    "    if with_shape: display(f'Tensor shape={img.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note - this won't work if you've already run the notebook all the way through, because `ll` now contains Tensors representing Spectrograms, not `(Signal, SampleRate)` tuples.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_spectro(speccer(ll.train[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create the transforms with the params we want, and rebuild our label lists using them. \n",
    "\n",
    "Note that now the items in the final `LabelList` won't be tuples anymore, they'll just be tensors. This is convenient for actually using the data, but it means you can't really go back and listen to your audio anymore. We can probably find a way around this, but let's press on for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_3sec = PadOrTrim(3000)\n",
    "speccer = Spectrogrammer(n_fft=1024, n_mels=64, top_db=80)\n",
    "\n",
    "tfms = [ToCuda(), pad_3sec, speccer]\n",
    "\n",
    "al = AudioList.from_files(path, tfms=tfms)\n",
    "sd = SplitData.split_by_func(al, partial(random_splitter, p_valid=0.2))\n",
    "ll = label_by_func(sd, speaker_labeler, proc_y=CategoryProcessor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_spectro(ll.train[4][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Databunch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we've got our beautifully transformed tensors, let's add them into a databunch, so we can feed a model easily.\n",
    "\n",
    "We can use our `get_dls` func which we defined in `03_minibatch_training`, but let's use the to_databunch func we defined in `08_data_block` instead, it's much nicer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs=64\n",
    "\n",
    "c_in = ll.train[0][0].shape[0]\n",
    "c_out = len(uniqueify(ll.train.y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ll.to_databunch(bs,c_in=c_in,c_out=c_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the dataloader's batching functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = next(iter(data.train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def show_batch(x, c=4, r=None, figsize=None, shower=show_image):\n",
    "    n = len(x)\n",
    "    if r is None: r = int(math.ceil(n/c))\n",
    "    if figsize is None: figsize=(c*3,r*3)\n",
    "    fig,axes = plt.subplots(r,c, figsize=figsize)\n",
    "    for xi,ax in zip(x,axes.flat): shower(xi, ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_spec_batch = partial(show_batch, c=4, r=2, figsize=None, \n",
    "                          shower=partial(show_spectro, with_shape=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_spec_batch(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go for gold! As a proof of concept, let's use the *pièce de résistance* learner builder with the hyperparameters from Lesson 11 `11_train_imagenette`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_func = adam_opt(mom=0.9, mom_sqr=0.99, eps=1e-6, wd=1e-2)\n",
    "loss_func = LabelSmoothingCrossEntropy()\n",
    "lr = 1e-2\n",
    "pct_start = 0.5\n",
    "phases = create_phases(pct_start)\n",
    "sched_lr  = combine_scheds(phases, cos_1cycle_anneal(lr/10., lr, lr/1e5))\n",
    "sched_mom = combine_scheds(phases, cos_1cycle_anneal(0.95,0.85, 0.95))\n",
    "cbscheds = [ParamScheduler('lr', sched_lr), \n",
    "            ParamScheduler('mom', sched_mom)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = cnn_learner(xresnet34, data, loss_func, opt_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(5, cbs=cbscheds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo - all at once"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is all the code it takes to do it end-to-end (not counting the `#export` cells above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dsid = \"ST-AEDS-20180100_1-OS\"\n",
    "# data_url = f'http://www.openslr.org/resources/45/{dsid}' # actual URL has .tgz extension but untar_data doesn't like that\n",
    "# path = Path.home() / Path(f\".fastai/data/{dsid}/\")\n",
    "# datasets.untar_data(data_url, dest=path)\n",
    "\n",
    "# pad_3sec = PadOrTrim(3000)\n",
    "# speccer = Spectrogrammer(n_fft=1024, n_mels=64, top_db=80)\n",
    "\n",
    "# tfms = [pad_3sec, speccer]\n",
    "\n",
    "# al = AudioList.from_files(path, tfms=tfms)\n",
    "\n",
    "# sd = SplitData.split_by_func(al, partial(random_splitter, p_valid=0.2))\n",
    "\n",
    "# label_pat = r'/([mf]\\d+)_'\n",
    "# speaker_labeler = partial(re_labeler, pat=label_pat)\n",
    "# ll = label_by_func(sd, speaker_labeler, proc_y=CategoryProcessor())\n",
    "\n",
    "# bs=64\n",
    "# c_in = ll.train[0][0].shape[0]\n",
    "# c_out = len(uniqueify(ll.train.y))\n",
    "\n",
    "# data = ll.to_databunch(bs,c_in=c_in,c_out=c_out)\n",
    "\n",
    "# opt_func = adam_opt(mom=0.9, mom_sqr=0.99, eps=1e-6, wd=1e-2)\n",
    "# loss_func = LabelSmoothingCrossEntropy()\n",
    "# lr = 1e-2\n",
    "# pct_start = 0.5\n",
    "# phases = create_phases(pct_start)\n",
    "# sched_lr  = combine_scheds(phases, cos_1cycle_anneal(lr/10., lr, lr/1e5))\n",
    "# sched_mom = combine_scheds(phases, cos_1cycle_anneal(0.95,0.85, 0.95))\n",
    "# cbscheds = [ParamScheduler('lr', sched_lr), \n",
    "#             ParamScheduler('mom', sched_mom)]\n",
    "\n",
    "# learn = cnn_learner(xresnet34, data, loss_func, opt_func)\n",
    "# learn.fit(5, cbs=cbscheds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_auto_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
